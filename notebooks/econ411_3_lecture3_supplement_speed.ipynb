{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb09a354",
   "metadata": {},
   "source": [
    "# 0 Overview\n",
    "This notebook is a supplement to our introductory notebook, `lecture 3 standard incomplete markets steady state slides.ipynb`, for more advanced users.\n",
    "\n",
    "Here, we'll *profile* the basic standard incomplete markets code we introduced in that notebook, to see where there are opportunities to speed it up. Then, we'll implement some key improvements. By the time we finish, the computation of a steady state will be approximately twice as fast as in the introductory notebook, and fully on par with the state of the art: 17 milliseconds to calculate the policy function and distribution (each on a state space of dimension 3500) on my run-of-the-mill Macbook Air laptop.\n",
    "\n",
    "The faster version of `sim_steady_state.py`, which uses the functions from this notebook, will be in `sim_steady_state_fast.py`, which is also an excellent place to look if you want to briefly see the changes that have been made.\n",
    "\n",
    "This notebook has 5 sections:\n",
    "\n",
    "**[Section 1: introduction to profiling](#section1)**. We'll discuss what it means to *profile* code, to see how long each chunk of code requires to run, and which parts of the code appear to be the major bottlenecks. As an example, we'll start to profile the major functions from our last notebook, which are included in `sim_steady_state.py` (imported as `sim`).\n",
    "\n",
    "**[Section 2: profiling and improving `distribution_ss`](#section2).** We'll profile `distribution_ss`, which solves for the steady-state distribution given a policy function, in detail. Using our insights from profiling, we'll make several specific improvements, which together will make `distribution_ss` approximately twice as fast.\n",
    "\n",
    "**[Section 3: profiling and improving `policy_ss`](#section3).** We'll profile `policy_ss`, which solves for the steady-state policy function given parameters. Combining information from profiling with insights from section 2, we'll speed up `policy_ss` by a factor of more than two.\n",
    "\n",
    "**[Section 4: overall steady-state performance](#section4).** We'll combine the improved versions of `distribution_ss` and `policy_ss` to obtain an updated `steady_state` function, which doubles the speed of our old one, calculating the steady-state policy and distribution in about 50 ms on an old laptop.\n",
    "\n",
    "**[Section 5 (addendum): replacing `get_lottery`](#section5).** Here we make one extra change, which will be useful later on. In section 2, we see that the `get_lottery` function from our previous notebook, which converts the continuous policy function into a lottery over adjacent gridpoints, is not very fast—but that this doesn't matter for computing the steady state, since then it only needs to be done once. Here, we use the improved interpolation code from section 3 to speed it up anyway, which will be invaluable as we move to dynamics.\n",
    "\n",
    "*Note*: although one important way to improve code performance is through [parallelization](https://en.wikipedia.org/wiki/Parallel_computing), we won't cover that in this notebook: our code will not be explicitly parallel in any way, though very small parts of it (e.g. matrix multiplication) may be automatically parallelized by NumPy. Parallelization could likely achieve significant additional gains.\n",
    "\n",
    "**Preliminaries.** Before we start, we'll import the basic Python libraries we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1876c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43d75d",
   "metadata": {},
   "source": [
    "and also the code developed in our previous notebook, along with the example calibration that is included with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf917630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sim_steady_state as sim\n",
    "calibration = sim.example_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62de27",
   "metadata": {},
   "source": [
    "You may recall that this calibration has a state space of dimension 3500: there are 500 gridpoints for the grid of assets, and 7 discrete income states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd514d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(calibration['a_grid']), len(calibration['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98543e38",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# 1 Introduction to profiling\n",
    "Broadly, to \"profile\" code is to analyze code to see what parts of the code take the most time, memory, and so on to run, and also which parts of the code are run the most.\n",
    "\n",
    "In this notebook, we'll be using the profiling tools that are available in Jupyter and its sibling IPython. (These tools can also be called from the Python language itself, though we will not do this.) We'll also focus on speed, rather than memory footprint. \n",
    "\n",
    "# 1.1 `%time` and `%timeit`\n",
    "The most basic profiling tools available in Jupyter/IPython are the \"magic\" commands `%time` and `%timeit`.\n",
    "\n",
    "The first of these, `%time`, reports the amount of time that it takes to run the line of code once. There are two measures, and we will focus on the second, \"wall time\", which is literally the amount of time that elapsed while running the line of code.\n",
    "\n",
    "(Technical notes: this can sometimes be too high if your computer gets distracted and also works hard on something else while running the line of code, in which case you might notice a big discrepancy with the other measure, \"CPU time\". \"CPU time\", on the other hand, can be higher if your code is parallelized, since it counts each core separately.)\n",
    "\n",
    "Let's run `%time` on our steady state function. The dict `calibration` contains exactly the inputs needed by that function, and feeding it in as an input with two stars, `**calibration`, tells Python to \"unpack\" the dict and feed its key-value pairs as arguments to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429cd494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 18.9 ms, total: 220 ms\n",
      "Wall time: 252 ms\n"
     ]
    }
   ],
   "source": [
    "%time ss = sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3594d21",
   "metadata": {},
   "source": [
    "That took a while, with wall time of more than a quarter of a second! Let's run it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682c286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 ms, sys: 853 µs, total: 33.9 ms\n",
      "Wall time: 33.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time ss = sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6d648",
   "metadata": {},
   "source": [
    "Now this is much quicker, with wall time for finding the steady state equal to only 1/30th of a second!\n",
    "\n",
    "This discrepancy is because our steady-state code contains a function, `forward_policy`, that is accelerated with [Numba](https://numba.pydata.org/). The first time that function is run, Numba needs to *compile* it, which requires a one-time delay, analogous to (but usually far quicker than) the up-front time needed for compilation in a language like Fortran or C.\n",
    "\n",
    "Usually, when profiling code, we want to ignore this compilation time: it's a one-time cost, and will quickly become irrelevant in comparison to speed improvements when we run our code enough times. (And if it doesn't become irrelevant, perhaps because the code is already fast enough, or we tend not to run it very many times in a session, then optimization with Numba was probably unnecessary in the first place.)\n",
    "\n",
    "Hence, we have learned our first lesson: *to make sure that our profiling is unaffected by Numba compilation time, run the code once before timing it whenever compilation might be an issue*. We'll call this a **burn-in** run.\n",
    "\n",
    "The other command, `%timeit`, is similar to `%time`, but runs a line of code many times to get a better estimate of its running time, and also does some other more sophisticated things to try and get as accurate an estimate as possible. It usually produces a lower and more accurate estimate than `%time`, but takes longer (and another disadvantage is that you can't use it to save anything to a variable). Let's try it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "571e52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.8 ms ± 39.4 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471c2e6",
   "metadata": {},
   "source": [
    "This 32 ms is only slightly lower than the 33 ms from `%time` above, so in this case there is little difference. The extra accuracy provided by `%timeit` tends to be more important when we are looking at functions that are very fast, with runtimes that are at most a few milliseconds, and maybe in the microseconds or nanoseconds.\n",
    "\n",
    "If we look at its source code in `sim_steady_state.py`, the two key calls made by the `steady_state` function are, first, to `policy_ss` to get the steady-state marginal value function and policy functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03938261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 ms, sys: 673 µs, total: 24.5 ms\n",
      "Wall time: 23.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time Va, a, c = sim.policy_ss(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc31b575",
   "metadata": {},
   "source": [
    "and then to `distribution_ss` to get the steady-state distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e014bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.81 ms, sys: 1.07 ms, total: 10.9 ms\n",
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%time D = sim.distribution_ss(calibration['Pi'], a, calibration['a_grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd365e61",
   "metadata": {},
   "source": [
    "We see that `policy_ss` is the more costly function here, but that `distribution_ss` also takes up a substantial amount of time.\n",
    "\n",
    "To delve further, we want to look and see what parts of the functions `policy_ss` and `distribution_ss` are most costly. We could continue to look at the source and run `%time` or `%timeit` on each line by hand, but there is a much more convenient way: using a *line profiling* tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bdff88",
   "metadata": {},
   "source": [
    "# 1.2 Line profiling\n",
    "Beyond `%time` and `%timeit`, the easiest-to-understand profiling tool available for Python is the *line profiler*.\n",
    "\n",
    "Unfortunately, this requires a separate installation. At the command line, you can install it with:\n",
    "```\n",
    "python -m pip install line_profiler\n",
    "```\n",
    "and then load it within a Jupyter or IPython session with the line\n",
    "```\n",
    "%load_ext line_profiler\n",
    "```\n",
    "So that this notebook is still usable even if you haven't installed the line profiler, I have commented out the lines that use it, and copy-pasted the results directly into the notebook. By uncommenting the lines, you can run the line profiler yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8773c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to load line profiler extension\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac0722",
   "metadata": {},
   "source": [
    "As an initial example, let's replicate what we already did manually with `%time`: looking at our `steady_state` function and seeing how long each step takes.\n",
    "\n",
    "To do so, we run the command `%lprun -f `, followed by, first, the function within which we want to see lines profiled (here, `sim.steady_state`), and second, the line of code we want to run (here, `sim.steady_state(**calibration)`).\n",
    "\n",
    "(To make the results readable, either zoom out in your browser or widen your browser window as much as possible!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad92db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# %lprun -f sim.steady_state sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25f1cd",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.052659 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/sim_steady_state.py\n",
    "Function: steady_state at line 173\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   173                                           def steady_state(Pi, a_grid, y, r, beta, eis):\n",
    "   174         1   40782000.0    4e+07     77.4      Va, a, c = policy_ss(Pi, a_grid, y, r, beta, eis)\n",
    "   175         1     266000.0 266000.0      0.5      a_i, a_pi = get_lottery(a, a_grid)\n",
    "   176         1   11598000.0    1e+07     22.0      D = distribution_ss(Pi, a, a_grid)\n",
    "   177                                               \n",
    "   178         2       4000.0   2000.0      0.0      return dict(D=D, Va=Va, \n",
    "   179         1          0.0      0.0      0.0                  a=a, c=c, a_i=a_i, a_pi=a_pi,\n",
    "   180         1       9000.0   9000.0      0.0                  A=np.vdot(a, D), C=np.vdot(c, D),\n",
    "   181         1          0.0      0.0      0.0                  Pi=Pi, a_grid=a_grid, y=y, r=r, beta=beta, eis=eis)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bc17f",
   "metadata": {},
   "source": [
    "This delivers a similar message to what we saw earlier: the `policy_ss` and `distribution_ss` functions take up essentially all the time in this function, with the split being approximately 3/4 for `policy_ss` and 1/4 for `distribution_ss`.\n",
    "\n",
    "The other information—how many times each line is run (\"hits\"), and then distinguishing between total time and time per hit—is not yet useful, since both `policy_ss` and `distribution_ss` are only run once.\n",
    "\n",
    "(Note that there is some overhead here, and the line profiler overstates the actual amount of time the function takes to run.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f3d9f",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# 2 Profiling and improving `distribution_ss`\n",
    "In the last section, we saw that about 75% of the time to find a steady state was in the `policy_ss` function that iterates backward to find the steady-state policy function, and 25% in the `distribution_ss` function that iterates forward to find the steady-state distribution.\n",
    "\n",
    "Although `distribution_ss` is the lesser of the two contributors to running time here, we'll start by speeding it up—both because it's a simpler warm-up exercise, and because it will teach us some useful lessons that will carry over to `policy_ss`.\n",
    "\n",
    "## 2.1 Profiling `distribution_ss`\n",
    "Let's run the line profiler on `distribution_ss`. For simplicity, we'll run the same command to obtain the whole steady state, `sim.steady_state(**calibration)`, but then ask the profiler to only look at the `distribution_ss` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3094c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# %lprun -f sim.distribution_ss sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63d618f",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.011969 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/sim_steady_state.py\n",
    "Function: distribution_ss at line 156\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   156                                           def distribution_ss(Pi, a, a_grid, tol=1E-10):\n",
    "   157         1     246000.0 246000.0      2.1      a_i, a_pi = get_lottery(a, a_grid)\n",
    "   158                                               \n",
    "   159                                               # as initial D, use stationary distribution for s, plus uniform over a\n",
    "   160         1    2167000.0    2e+06     18.1      pi = stationary_markov(Pi)\n",
    "   161         1      20000.0  20000.0      0.2      D = pi[:, np.newaxis] * np.ones_like(a_grid) / len(a_grid)\n",
    "   162                                               \n",
    "   163                                               # now iterate until convergence to acceptable threshold\n",
    "   164       581      91000.0    156.6      0.8      for _ in range(10_000):\n",
    "   165       581    6303000.0  10848.5     52.7          D_new = forward_iteration(D, Pi, a_i, a_pi)\n",
    "   166       581    3034000.0   5222.0     25.3          if np.max(np.abs(D_new - D)) < tol:\n",
    "   167         1          0.0      0.0      0.0              return D_new\n",
    "   168       580     108000.0    186.2      0.9          D = D_new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297dafb",
   "metadata": {},
   "source": [
    "Note that the \"timer unit\" of 1e-09 s means that the line times in this report are in *nanoseconds*, i.e. billionths of a second.\n",
    "\n",
    "We can draw 4 lessons from the above:\n",
    "\n",
    "1. The single biggest cost, as expected, is running the actual forward iteration on the distribution, line 165, 581 times. This takes 50% of runtime.\n",
    "\n",
    "\n",
    "2. But there is a shockingly high cost from line 166, which simply *checks whether the convergence threshold has been reached*. At 25% of runtime, this is half the cost of the actual forward iteration!\n",
    "\n",
    "\n",
    "3. There is also a pretty high cost from line 160, which runs `stationary_markov` to obtain the stationary distribution for the exogenous Markov process itself, which is then used to construct a nicer initial guess for the distribution.  This line wasn't strictly necessary, and we included it to reduce the number of iterations a little bit—but at 18% of runtime, it's probably not worth it.\n",
    "\n",
    "\n",
    "4. Line 156, which runs `get_lottery` to find the \"lottery\" representation of the policy function, is insignificant here, at only 2% of runtime for the steady state. But that's because we only need to run it once, on the steady-state policy function, rather than separately for each iteration. Per run, it actually has a much higher cost than our forward iteration function (246 vs. 11 microseconds). In a dynamic context, where the policy function is changing each period and we'd need to rerun this line, it would become a major part of the cost.\n",
    "\n",
    "\n",
    "Let's follow up on point 3, and both time and profile the `stationary_markov` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a692271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25 ms ± 9.96 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sim.stationary_markov(calibration['Pi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce31a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# %lprun -f sim.stationary_markov sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04439a8",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.002397 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/sim_steady_state.py\n",
    "Function: stationary_markov at line 51\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    51                                           def stationary_markov(Pi, tol=1E-14):\n",
    "    52                                               # start with uniform distribution over all states\n",
    "    53         1       2000.0   2000.0      0.1      n = Pi.shape[0]\n",
    "    54         1       9000.0   9000.0      0.4      pi = np.full(n, 1/n)\n",
    "    55                                               \n",
    "    56                                               # update distribution using Pi until successive iterations differ by less than tol\n",
    "    57       556      89000.0    160.1      3.7      for _ in range(10_000):\n",
    "    58       556     510000.0    917.3     21.3          pi_new = Pi.T @ pi\n",
    "    59       556    1687000.0   3034.2     70.4          if np.max(np.abs(pi_new - pi)) < tol:\n",
    "    60         1          0.0      0.0      0.0              return pi_new\n",
    "    61       555     100000.0    180.2      4.2          pi = pi_new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8b358",
   "metadata": {},
   "source": [
    "This is an alarming echo of point 2 from above: the vast majority of the time is spent on line 59, which simply checks whether or not there has been convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064a53d",
   "metadata": {},
   "source": [
    "## 2.2 Speeding up `stationary_markov`\n",
    "First, let's improve `stationary_markov`, directly addressing point 3 from above, and in the process learning how to address point 2.\n",
    "\n",
    "The key is to avoid all the time spent in line 59, which checks whether there has been convergence. The simplest way to do this is to simply *cut down the number of times* this test is run. Given that it takes 556 iterations to achieve convergence, testing on every single iteration seems excessive. Why not test every 10 iterations? We'll end up doing a few more iterations than needed, but vastly cut down on the test time, which is the dominant cost here.\n",
    "\n",
    "Below we rewrite the function to achieve this, only performing the comparison when the iteration counter `it` is a multiple of 10 (i.e. when `it % 10 == 0`, where `%` is the modulo function). Here we're making use of the \"short-circuit\" feature of Python `and`: if the first argument evaluates as false, the second one isn't even calculated.\n",
    "\n",
    "Also note that we're using `%%writefile` to write this cell to a file, since the line profiler works better when a function is from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d95b71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%writefile temp1.py\n",
    "# uncomment above to write file if you're using line profiler yourself\n",
    "import numpy as np\n",
    "def stationary_markov(Pi, tol=1E-14):\n",
    "    # start with uniform distribution over all states\n",
    "    n = Pi.shape[0]\n",
    "    pi = np.full(n, 1/n)\n",
    "    \n",
    "    # update distribution using Pi until successive iterations differ by less than tol\n",
    "    for it in range(10_000):\n",
    "        pi_new = Pi.T @ pi\n",
    "        if it % 10 == 0 and np.max(np.abs(pi_new - pi)) < tol:\n",
    "            return pi_new\n",
    "        pi = pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f891435",
   "metadata": {},
   "source": [
    "Now let's profile this new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0233afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# import temp1\n",
    "# temp1.stationary_markov(calibration['Pi']) # burn-in new module\n",
    "# %lprun -f temp1.stationary_markov temp1.stationary_markov(calibration['Pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006d449",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.000888 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/temp1.py\n",
    "Function: stationary_markov at line 3\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     3                                           def stationary_markov(Pi, tol=1E-14):\n",
    "     4                                               # start with uniform distribution over all states\n",
    "     5         1       2000.0   2000.0      0.2      n = Pi.shape[0]\n",
    "     6         1      27000.0  27000.0      3.0      pi = np.full(n, 1/n)\n",
    "     7                                               \n",
    "     8                                               # update distribution using Pi until successive iterations differ by less than tol\n",
    "     9       561      69000.0    123.0      7.8      for it in range(10_000):\n",
    "    10       561     434000.0    773.6     48.9          pi_new = Pi.T @ pi\n",
    "    11       561     282000.0    502.7     31.8          if it % 10 == 0 and np.max(np.abs(pi_new - pi)) < tol:\n",
    "    12         1          0.0      0.0      0.0              return pi_new\n",
    "    13       560      74000.0    132.1      8.3          pi = pi_new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bd38a",
   "metadata": {},
   "source": [
    "Now the total time has fallen substantially, and the line that checks for convergence accounts for a smaller share of total runtime.\n",
    "\n",
    "Indeed, if we use `%timeit` to get a very accurate measurement of the total time needed to run this function, we see that it's already very low, just 430 µs—a factor-of-3 improvement from our earlier function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9974400c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431 µs ± 294 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit stationary_markov(calibration['Pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f472e4",
   "metadata": {},
   "source": [
    "**Making the convergence test even cheaper.** This is already good enough for most practical purposes, but we can make one nice additional improvement.\n",
    "\n",
    "Using `np.max(np.abs(pi_new - pi)) < tol` to check whether the arrays are equal to within a tolerance is highly inefficient: this command first constructs two intermediate arrays (`pi_new - pi`, and then its absolute value) and then takes a maximum before comparing to the tolerance. Most of the time, it will suffice simply to compare one or two entries of `pi_new` and `pi` and see that we're not yet within the tolerance. It's also always good to avoid creation of intermediate arrays in memory, which is costly (and a much bigger deal for larger arrays). We can write a very simple Numba-accelerated function called `equal_tolerance` that does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dff1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def equal_tolerance(x1, x2, tol):\n",
    "    # \"ravel\" flattens both x1 and x2, without making copies, so we can compare the\n",
    "    # with a single for loop even if they have multiple dimensions. not needed for now,\n",
    "    # but will be useful later!\n",
    "    x1 = x1.ravel()\n",
    "    x2 = x2.ravel()\n",
    "    for i in range(len(x1)):\n",
    "        if np.abs(x1[i] - x2[i]) >= tol:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271c880",
   "metadata": {},
   "source": [
    "and call it in the new `stationary_markov` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec8d8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationary_markov(Pi, tol=1E-14):\n",
    "    # start with uniform distribution over all states\n",
    "    n = Pi.shape[0]\n",
    "    pi = np.full(n, 1/n)\n",
    "    \n",
    "    # update distribution using Pi until successive iterations differ by less than tol\n",
    "    for it in range(10_000):\n",
    "        pi_new = Pi.T @ pi\n",
    "        if it % 10 == 0 and equal_tolerance(pi, pi_new, tol):\n",
    "            return pi_new\n",
    "        pi = pi_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a54bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342 µs ± 638 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "stationary_markov(calibration['Pi']) # burn-in\n",
    "%timeit stationary_markov(calibration['Pi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96b522",
   "metadata": {},
   "source": [
    "This change has brought the runtime down from 1.24 ms to 340 µs, a decrease of over 70%. At this point, `stationary_markov` is certainly no longer a major bottleneck.\n",
    "\n",
    "**Alternative: dominant eigenvector.** One can achieve even more impressive speeds using SciPy's built-in eigenvalue and eigenvector solver to look for the dominant left eigenvector of `Pi`. We'll use `%%timeit`, a generalization of `%timeit` that times an entire cell, to time this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd2bfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 µs ± 57.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "w, v = linalg.eig(calibration['Pi'].T) # to get left eigenvectors, take transpose of Pi\n",
    "pi = v[:, np.argmax(w)].real  # take eigenvector for maximum eigenvalue, ignore imaginary part (numerical error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4339968f",
   "metadata": {},
   "source": [
    "This delivers almost another 20-fold improvement—but from a very low base already. We won't use this approach, because it doesn't generalize as well when `Pi` is not a simple matrix anymore (and instead is sparse, or a product of several independent matrices, etc). It is also possible to obtain comparable speed gains by compiling `stationary_markov` with Numba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9713f",
   "metadata": {},
   "source": [
    "## 2.3 Speeding up `distribution_ss`\n",
    "With the insights from working on `stationary_markov`, we are now ready to improve `distribution_ss`. Right now, a precise timing says that the original function takes 9 ms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccb96625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.75 ms ± 8.23 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sim.distribution_ss(calibration['Pi'], a, calibration['a_grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca68196",
   "metadata": {},
   "source": [
    "Let's rewrite `distribution_ss` to use the new `stationary_markov`, to only test for convergence every 10th iteration, and also to use `equal_tolerance` for those tests, just like in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d87299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_ss(Pi, a, a_grid, tol=1E-10):\n",
    "    a_i, a_pi = sim.get_lottery(a, a_grid)\n",
    "    \n",
    "    # as initial D, use stationary distribution for s, plus uniform over a\n",
    "    pi = stationary_markov(Pi)\n",
    "    D = pi[:, np.newaxis] * np.ones_like(a_grid) / len(a_grid)\n",
    "    \n",
    "    # now iterate until convergence to acceptable threshold\n",
    "    for it in range(10_000):\n",
    "        D_new = sim.forward_iteration(D, Pi, a_i, a_pi)\n",
    "        if it % 10 == 0 and equal_tolerance(D_new, D, tol):\n",
    "            return D_new\n",
    "        D = D_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224fe5c",
   "metadata": {},
   "source": [
    "How much of an improvement did we get from these? A one-third reduction in runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b017c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.81 ms ± 37.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "distribution_ss(calibration['Pi'], a, calibration['a_grid']) # burn-in\n",
    "%timeit distribution_ss(calibration['Pi'], a, calibration['a_grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f08860",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# 3 Profiling and improving `policy_ss`\n",
    "Now we move to the more major project: more quickly obtaining the steady-state policy function in `policy_ss`. Start by running the line profiler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55fce9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# %lprun -f sim.policy_ss sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07486a2",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.030549 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/sim_steady_state.py\n",
    "Function: policy_ss at line 108\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   108                                           def policy_ss(Pi, a_grid, y, r, beta, eis, tol=1E-9):\n",
    "   109                                               # initial guess for Va: assume consumption 5% of cash-on-hand, then get Va from envelope condition\n",
    "   110         1      36000.0  36000.0      0.1      coh = y[:, np.newaxis] + (1+r)*a_grid\n",
    "   111         1       4000.0   4000.0      0.0      c = 0.05 * coh\n",
    "   112         1       7000.0   7000.0      0.0      Va = (1+r) * c**(-1/eis)\n",
    "   113                                               \n",
    "   114                                               # iterate until maximum distance between two iterations falls below tol, fail-safe max of 10,000 iterations\n",
    "   115       541      82000.0    151.6      0.3      for it in range(10_000):\n",
    "   116       541   27248000.0  50366.0     89.2          Va, a, c = backward_iteration(Va, Pi, a_grid, y, r, beta, eis)\n",
    "   117                                                   \n",
    "   118                                                   # after iteration 0, can compare new policy function to old one\n",
    "   119       541    3091000.0   5713.5     10.1          if it > 0 and np.max(np.abs(a - a_old)) < tol:\n",
    "   120         1          0.0      0.0      0.0              return Va, a, c\n",
    "   121                                                   \n",
    "   122       540      81000.0    150.0      0.3          a_old = a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b231c88",
   "metadata": {},
   "source": [
    "The message here is quite clear: a small amount of the total runtime (10%) is in the convergence check, which we already know how to improve, and essentially all the rest is in the `backward_iteration` function\n",
    "\n",
    "Let's turn to that function, and run the line profiler, next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e265b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to run line profiler yourself, if you've loaded extension with %load_ext line_profiler\n",
    "# %lprun -f sim.backward_iteration sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64589f98",
   "metadata": {},
   "source": [
    "```\n",
    "Timer unit: 1e-09 s\n",
    "\n",
    "Total time: 0.027014 s\n",
    "File: /Users/matthewrognlie/Dropbox/Courses/Northwestern grad/Econ 411-3 Macro 2024/sim_steady_state.py\n",
    "Function: backward_iteration at line 86\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "    86                                           def backward_iteration(Va, Pi, a_grid, y, r, beta, eis):\n",
    "    87                                               # step 1: discounting and expectations\n",
    "    88       541    2900000.0   5360.4     10.7      Wa = beta * Pi @ Va\n",
    "    89                                               \n",
    "    90                                               # step 2: solving for asset policy using the first-order condition\n",
    "    91       541     655000.0   1210.7      2.4      c_endog = Wa**(-eis)\n",
    "    92       541    1976000.0   3652.5      7.3      coh = y[:, np.newaxis] + (1+r)*a_grid\n",
    "    93                                               \n",
    "    94       541     287000.0    530.5      1.1      a = np.empty_like(coh)\n",
    "    95      4328     581000.0    134.2      2.2      for e in range(len(y)):\n",
    "    96      3787   17611000.0   4650.4     65.2          a[e, :] = np.interp(coh[e, :], c_endog[e, :] + a_grid, a_grid)\n",
    "    97                                                   \n",
    "    98                                               # step 3: enforcing the borrowing constraint and backing out consumption\n",
    "    99       541    1073000.0   1983.4      4.0      a = np.maximum(a, a_grid[0])\n",
    "   100       541     624000.0   1153.4      2.3      c = coh - a\n",
    "   101                                               \n",
    "   102                                               # step 4: using the envelope condition to recover the derivative of the value function\n",
    "   103       541    1227000.0   2268.0      4.5      Va = (1+r) * c**(-1/eis)\n",
    "   104                                               \n",
    "   105       541      80000.0    147.9      0.3      return Va, a, c\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff23e3e",
   "metadata": {},
   "source": [
    "By far the highest cost here is in the linear interpolation in line 96. Speedup of this is clearly the highest priority. Note that a separate call to `np.interp` is made for every exogenous state `e`, which may be part of the cost here (also, using `for` to loop over a line in pure Python tends to be quite costly in general).\n",
    "\n",
    "One additional useful observation is that the points in the array `coh[e, :]` are increasing. `np.interp`, which only requires that the data points `c_endog[e, :]` are increasing, does not know to take advantage of this. It's a useful fact, because it means we don't need to search the entire array `c_endog[e, :]` each time to locate a point in `coh[e, :]`; instead, we can just look to the right of where we found the last point.\n",
    "\n",
    "Let's write a Numba-accelerated function that uses this insight to do interpolation, as an alternative version of `np.interp`. It's a bit subtle, so we won't dwell on exactly how it works (which, unlike the other points we've discussed, does not have the same broad applicability) except in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e60cd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def interpolate_monotonic(x, xp, yp):\n",
    "    \"\"\"Linearly interpolate the data points (xp, yp) and evaluate at x\"\"\"\n",
    "    nx, nxp = x.shape[0], xp.shape[0]\n",
    "    y = np.empty(nx)\n",
    "    \n",
    "    # at any given moment, we are looking between data points with indices xp_i and xp_i+1\n",
    "    # we'll keep track of xp_i and also the values of xp at xp_i and xp_i+1\n",
    "    # (note: if x is outside range of xp, we'll use closest data points and extrapolate)\n",
    "    xp_i = 0\n",
    "    xp_lo = xp[xp_i]\n",
    "    xp_hi = xp[xp_i + 1]\n",
    "    \n",
    "    # iterate through all points in x\n",
    "    for xi_cur in range(nx):\n",
    "        x_cur = x[xi_cur]\n",
    "        while xp_i < nxp - 2:\n",
    "            # if current x (x_cur) is below upper data point (xp_hi), we're good\n",
    "            if x_cur < xp_hi:\n",
    "                break\n",
    "                \n",
    "            # otherwise, we need to look at the next pair of data points until x_cur is less than xp_hi\n",
    "            # (so between xp_lo and xp_hi)\n",
    "            xp_i += 1\n",
    "            xp_lo = xp_hi\n",
    "            xp_hi = xp[xp_i + 1]\n",
    "\n",
    "        # find the pi such that x_cur = pi*x_lo + (1-pi)*x_hi\n",
    "        pi = (xp_hi - x_cur) / (xp_hi - xp_lo)\n",
    "        \n",
    "        # use this pi to interpolate the y\n",
    "        y[xi_cur] = pi * yp[xp_i] + (1 - pi) * yp[xp_i + 1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3a6cac",
   "metadata": {},
   "source": [
    "Let's quickly check that on sample data, this gives the same answer as `np.interp` up to numerical error. (Important side note: this actually will differ from `np.interp` if some `x` are outside the range of the `xp`, since it will linearly extrapolate, whereas `np.interp` just uses the nearest point. Since we prefer the extrapolation—although it should generally not be necesasry—this is actually desirable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a9632a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1102230246251565e-16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(1, 3, 20)\n",
    "xp = np.linspace(0, 4, 20)\n",
    "yp = np.random.rand(20)\n",
    "np.max(np.abs(interpolate_monotonic(x, xp, yp) - np.interp(x, xp, yp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ea393",
   "metadata": {},
   "source": [
    "Since the `for` loop in pure Python is also costly, let's write a wrapper around this that iterates over the outer dimension (except for the third argument, where we don't need that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "add0b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def interpolate_monotonic_loop(x, xp, yp):\n",
    "    ne = x.shape[0]\n",
    "    y = np.empty_like(x)\n",
    "    for e in range(ne):\n",
    "        y[e, :] = interpolate_monotonic(x[e, :], xp[e, :], yp)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df4f75",
   "metadata": {},
   "source": [
    "Let's gauge the overall speed improvement on data with the same dimensions as ours, where we'll make up the `c_endog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "171148b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_grid = calibration['a_grid']\n",
    "y = calibration['y']\n",
    "coh = y[:, np.newaxis] + (1+calibration['r'])*a_grid\n",
    "c_endog = 0.05 * coh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369da98",
   "metadata": {},
   "source": [
    "How long does this interpolation take using the old method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21ec7501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 µs ± 276 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "a = np.empty_like(coh)\n",
    "for e in range(len(y)):\n",
    "    a[e, :] = np.interp(coh[e, :], c_endog[e, :] + a_grid, a_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c7ad2",
   "metadata": {},
   "source": [
    "What about the new method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8959f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.51 µs ± 46.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "interpolate_monotonic_loop(coh, c_endog + a_grid, a_grid) # burn-in\n",
    "%timeit interpolate_monotonic_loop(coh, c_endog + a_grid, a_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37b069",
   "metadata": {},
   "source": [
    "A big improvement, by almost a factor of three!\n",
    "\n",
    "**One minor improvement: enforcing the borrowing constraint better.** The line profiling of `backward_iteration` above does not reveal many other places where much improvement is possible. The only line that slightly sticks out is line 99, where the borrowing constraint is enforced by writing `a = np.maximum(a, a_grid[0])`.\n",
    "\n",
    "This is an inefficient way to update `a` for two reasons. First, it creates an entirely new array in memory, rather than just altering `a` in place. Second, it compares every single entry of `a` to `a_grid[0]`, when the only entries of `a[e, :]` that are likely to violate the borrowing constraint, for each `e`, are a few early entries. Once we find one that is above `a_grid[0]`, the monotonicity of `a` means that we can stop looking.\n",
    "\n",
    "Let's write a simple Numba-accelerated function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6bca739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def setmin(x, xmin):\n",
    "    \"\"\"Set 2-dimensional array x, where each row is ascending, equal to max(x, xmin).\"\"\"\n",
    "    ni, nj = x.shape\n",
    "    for i in range(ni):\n",
    "        for j in range(nj):\n",
    "            if x[i, j] < xmin:\n",
    "                # if below minimum, enforce minimum\n",
    "                x[i, j] = xmin\n",
    "            else:\n",
    "                # otherwise, do nothing, and skip to next row (thanks to monotonicity)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546474e1",
   "metadata": {},
   "source": [
    "**Putting it all together.** We'll now write a new `backward_iteration` function that replaces our previous interpolation and constraint enforcement with the new functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84f31654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_iteration(Va, Pi, a_grid, y, r, beta, eis):\n",
    "    # step 1: discounting and expectations\n",
    "    Wa = beta * Pi @ Va\n",
    "    \n",
    "    # step 2: solving for asset policy using the first-order condition\n",
    "    c_endog = Wa**(-eis)\n",
    "    coh = y[:, np.newaxis] + (1+r)*a_grid\n",
    "    a = interpolate_monotonic_loop(coh, c_endog + a_grid, a_grid)\n",
    "        \n",
    "    # step 3: enforcing the borrowing constraint and backing out consumption\n",
    "    setmin(a, a_grid[0])\n",
    "    c = coh - a\n",
    "    \n",
    "    # step 4: using the envelope condition to recover the derivative of the value function\n",
    "    Va = (1+r) * c**(-1/eis)\n",
    "    \n",
    "    return Va, a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7f3240",
   "metadata": {},
   "source": [
    "Let's time the original version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6798432d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.1 µs ± 280 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "backward_inputs = ss['Va'], ss['Pi'], ss['a_grid'], ss['y'], ss['r'], ss['beta'], ss['eis']\n",
    "%timeit sim.backward_iteration(*backward_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62718a",
   "metadata": {},
   "source": [
    "and now the new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efca7abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.7 µs ± 414 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "backward_iteration(*backward_inputs) # burn-in\n",
    "%timeit backward_iteration(*backward_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44695a",
   "metadata": {},
   "source": [
    "We've sped this up by a factor of slightly less than 2. Now let's place this as part of a new `policy_ss` function, where we also eliminate most of the cost of checking for convergence through the same techniques as in the last section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9bfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_ss(Pi, a_grid, y, r, beta, eis, tol=1E-9):\n",
    "    # initial guess for Va: assume consumption 5% of cash-on-hand, then get Va from envelope condition\n",
    "    coh = y[:, np.newaxis] + (1+r)*a_grid\n",
    "    c = 0.05 * coh\n",
    "    Va = (1+r) * c**(-1/eis)\n",
    "    \n",
    "    # iterate until maximum distance between two iterations falls below tol, fail-safe max of 10,000 iterations\n",
    "    for it in range(10_000):\n",
    "        Va, a, c = backward_iteration(Va, Pi, a_grid, y, r, beta, eis)\n",
    "        \n",
    "        # after iteration 0, can compare new policy function to old one\n",
    "        if it % 10 == 1 and equal_tolerance(a, a_old, tol):\n",
    "            return Va, a, c\n",
    "        \n",
    "        a_old = a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf44b9",
   "metadata": {},
   "source": [
    "Time the old `policy_ss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "512c1aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sim.policy_ss(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314480e0",
   "metadata": {},
   "source": [
    "and now the new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8fd32b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.1 ms ± 70.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "policy_ss(**calibration) # burn-in (just in case)\n",
    "%timeit policy_ss(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f0a99",
   "metadata": {},
   "source": [
    "We've achieved a speed-up of slightly more than 2x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a6d8f9",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# 4 Overall steady-state performance\n",
    "Timing the old steady-state function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9960cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.8 ms ± 86.7 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sim.steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e0a2d",
   "metadata": {},
   "source": [
    "and now writing a new one that calls upon the functions we've defined in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c06ba381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def steady_state(Pi, a_grid, y, r, beta, eis):\n",
    "    Va, a, c = policy_ss(Pi, a_grid, y, r, beta, eis)\n",
    "    D = distribution_ss(Pi, a, a_grid)\n",
    "    \n",
    "    return dict(D=D, Va=Va, \n",
    "                a=a, c=c,\n",
    "                A=np.vdot(a, D), C=np.vdot(c, D),\n",
    "                Pi=Pi, a_grid=a_grid, y=y, r=r, beta=beta, eis=eis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d79ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 ms ± 88.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "steady_state(**calibration) # burn-in (just in case)\n",
    "%timeit steady_state(**calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d9c68",
   "metadata": {},
   "source": [
    "The overall improvement in performance is a factor of almost 2. At 17 ms on an ordinary laptop, the time to calculate a steady state for a model with a $7\\times 500=3500$-dimensional steady state is on par with the current state of the art."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56134fce",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# 5 Addendum: replacing `get_lottery`\n",
    "The current `get_lottery` function converts the asset policy, which generally takes a value between gridpoints, to a lottery over gridpoints.\n",
    "\n",
    "As we pointed out after the line profiling in section 2, this does not impose much of a cost in the steady state since it only needs to be done once, but in dynamic applications it will need to be applied every period, and the current `get_lottery` is surprisingly costly for that purpose.\n",
    "\n",
    "Fortunately, the `interpolate_monotonic` function that we defined above does, as an intermediate step, exactly what we want for `get_lottery`. We'll copy most of `interpolate_monotonic` below, and modify it so that the $i$ and $\\pi$ are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4bf0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def interpolate_lottery(x, xp):\n",
    "    \"\"\"Given a grid of xp, for each entry x_cur in (increasing) x, find the i and pi\n",
    "    such that x_cur = pi*xp[i] + (1-pi)*xp[i+1], where xp[i] and xp[i+1] bracket x_cur\"\"\"\n",
    "    nx, nxp = x.shape[0], xp.shape[0]\n",
    "    i = np.empty(nx, dtype=np.int64)\n",
    "    pi = np.empty(nx)\n",
    "    \n",
    "    # at any given moment, we are looking between data points with indices xp_i and xp_i+1\n",
    "    # we'll keep track of xp_i and also the values of xp at xp_i and xp_i+1\n",
    "    # (note: if x is outside range of xp, we'll use closest data points and extrapolate)\n",
    "    xp_i = 0\n",
    "    xp_lo = xp[xp_i]\n",
    "    xp_hi = xp[xp_i + 1]\n",
    "    \n",
    "    # iterate through all points in x\n",
    "    for xi_cur in range(nx):\n",
    "        x_cur = x[xi_cur]\n",
    "        while xp_i < nxp - 2:\n",
    "            # if current x (x_cur) is below upper data point (xp_hi), we're good\n",
    "            if x_cur < xp_hi:\n",
    "                break\n",
    "                \n",
    "            # otherwise, we need to look at the next pair obf data points until x_cur is less than xp_hi\n",
    "            # (so between xp_lo and xp_hi)\n",
    "            xp_i += 1\n",
    "            xp_lo = xp_hi\n",
    "            xp_hi = xp[xp_i + 1]\n",
    "\n",
    "        # find the pi such that x_cur = pi*x_lo + (1-pi)*x_hi\n",
    "        i[xi_cur] = xp_i\n",
    "        pi[xi_cur] = (xp_hi - x_cur) / (xp_hi - xp_lo)\n",
    "    return i, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632d9f4",
   "metadata": {},
   "source": [
    "and as we did before with `interpolate_monotonic`, we'll make a loop around this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d399dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def interpolate_lottery_loop(x, xp):\n",
    "    i = np.empty_like(x, dtype=np.int64)\n",
    "    pi = np.empty_like(x)\n",
    "    for e in range(x.shape[0]):\n",
    "        i[e, :], pi[e, :] = interpolate_lottery(x[e, :], xp)\n",
    "    return i, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a908891f",
   "metadata": {},
   "source": [
    "Let's make sure that this gives the same answer as `get_lottery` from before—in this case exactly the same, without even small numerical differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc41bace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i, pi = interpolate_lottery_loop(a, a_grid)\n",
    "i2, pi2 = sim.get_lottery(a, a_grid)\n",
    "np.all(i == i2), np.all(pi == pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643db28",
   "metadata": {},
   "source": [
    "What about the relative speeds? We've cut by a factor of around 20, which will be very useful when we need to apply this repeatedly in dynamic applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c18cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.49 µs ± 52 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      "203 µs ± 3.75 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit interpolate_lottery_loop(a, a_grid)\n",
    "%timeit sim.get_lottery(a, a_grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
